# 8.3 í”„ë¡œì íŠ¸ 3: í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œìŠ¤í…œ

ì»´í“¨í„° ë¹„ì „ê³¼ ëª¨ì…˜ í”Œë˜ë‹ì„ ê²°í•©í•œ ì™„ì „í•œ í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œìŠ¤í…œ êµ¬í˜„ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ

- ë¬¼ì²´ ê°ì§€ ë° ì¸ì‹ ì‹œìŠ¤í…œ êµ¬ì¶•
- 3D í¬ì¦ˆ ì¶”ì • êµ¬í˜„
- ëª¨ì…˜ í”Œë˜ë‹ ë° ê¶¤ì  ìƒì„±
- ê·¸ë¦¬í¼ ì œì–´ ìµœì í™”
- ì™„ì „ ìë™í™”ëœ í”½ì•¤í”Œë ˆì´ìŠ¤ íŒŒì´í”„ë¼ì¸

**ë‚œì´ë„**: â­â­â­ (ê³ ê¸‰)  
**ì†Œìš” ì‹œê°„**: 4ì‹œê°„  
**ì„ ìˆ˜ ì§€ì‹**: 1-5ì¥

---

## 1. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 1.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œìŠ¤í…œ                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  1. ì¸ì‹ ë‹¨ê³„                                        â”‚
â”‚     â”œâ”€ ì¹´ë©”ë¼ ì…ë ¥                                   â”‚
â”‚     â”œâ”€ ë¬¼ì²´ ê°ì§€ (YOLO/Detectron2)                  â”‚
â”‚     â”œâ”€ 3D í¬ì¦ˆ ì¶”ì • (Depth + RGB)                   â”‚
â”‚     â””â”€ ë¬¼ì²´ ë¶„ë¥˜ ë° ì„ íƒ                             â”‚
â”‚                                                     â”‚
â”‚  2. í”Œë˜ë‹ ë‹¨ê³„                                      â”‚
â”‚     â”œâ”€ ì ‘ê·¼ ê²½ë¡œ ê³„íš                                â”‚
â”‚     â”œâ”€ ê·¸ë˜ìŠ¤í•‘ í¬ì¸íŠ¸ ê³„ì‚°                          â”‚
â”‚     â”œâ”€ ì¶©ëŒ íšŒí”¼ ê²½ë¡œ ìƒì„±                           â”‚
â”‚     â””â”€ ê¶¤ì  ìµœì í™”                                   â”‚
â”‚                                                     â”‚
â”‚  3. ì‹¤í–‰ ë‹¨ê³„                                        â”‚
â”‚     â”œâ”€ ì ‘ê·¼ (Approach)                              â”‚
â”‚     â”œâ”€ ê·¸ë˜ìŠ¤í•‘ (Grasp)                             â”‚
â”‚     â”œâ”€ ë¦¬í”„íŠ¸ (Lift)                                â”‚
â”‚     â”œâ”€ ì´ë™ (Transfer)                              â”‚
â”‚     â”œâ”€ í”Œë ˆì´ìŠ¤ (Place)                             â”‚
â”‚     â””â”€ ë³µê·€ (Retreat)                               â”‚
â”‚                                                     â”‚
â”‚  4. ê²€ì¦ ë‹¨ê³„                                        â”‚
â”‚     â”œâ”€ ê·¸ë˜ìŠ¤í•‘ ì„±ê³µ í™•ì¸                            â”‚
â”‚     â”œâ”€ ìœ„ì¹˜ ì •í™•ë„ ê²€ì¦                              â”‚
â”‚     â””â”€ ì˜¤ë¥˜ ì²˜ë¦¬                                     â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 í•µì‹¬ ì»´í¬ë„ŒíŠ¸

**ì‹œìŠ¤í…œ êµ¬ì¡°**
```python
import numpy as np
import cv2
from dataclasses import dataclass
from typing import List, Tuple, Optional
from enum import Enum

class PickPlaceState(Enum):
    """í”½ì•¤í”Œë ˆì´ìŠ¤ ìƒíƒœ"""
    IDLE = 0
    DETECTING = 1
    PLANNING = 2
    APPROACHING = 3
    GRASPING = 4
    LIFTING = 5
    TRANSFERRING = 6
    PLACING = 7
    RETREATING = 8
    COMPLETED = 9
    FAILED = 10

@dataclass
class DetectedObject:
    """ê°ì§€ëœ ë¬¼ì²´ ì •ë³´"""
    class_name: str
    confidence: float
    bbox_2d: np.ndarray  # [x1, y1, x2, y2]
    position_3d: np.ndarray  # [x, y, z]
    orientation_3d: np.ndarray  # rotation matrix or quaternion
    dimensions: np.ndarray  # [width, height, depth]
    grasp_points: List[np.ndarray]  # possible grasp positions
    
@dataclass
class GraspPose:
    """ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ"""
    position: np.ndarray  # [x, y, z]
    orientation: np.ndarray  # rotation matrix
    approach_vector: np.ndarray  # approach direction
    grasp_width: float  # gripper opening
    quality_score: float  # grasp quality

@dataclass
class Trajectory:
    """ë¡œë´‡ ê¶¤ì """
    waypoints: List[np.ndarray]  # joint angles
    timestamps: List[float]
    velocities: List[np.ndarray]
    accelerations: List[np.ndarray]

class PickAndPlaceSystem:
    """í”½ì•¤í”Œë ˆì´ìŠ¤ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self, robot, camera):
        self.robot = robot
        self.camera = camera
        
        # í•˜ìœ„ ì‹œìŠ¤í…œë“¤
        self.detector = ObjectDetector()
        self.pose_estimator = PoseEstimator()
        self.grasp_planner = GraspPlanner()
        self.motion_planner = MotionPlanner(robot)
        
        # ìƒíƒœ
        self.state = PickPlaceState.IDLE
        self.current_object: Optional[DetectedObject] = None
        self.current_grasp: Optional[GraspPose] = None
        self.current_trajectory: Optional[Trajectory] = None
        
        # ì„¤ì •
        self.config = {
            'approach_distance': 0.10,  # 10cm
            'lift_height': 0.15,        # 15cm
            'grasp_force': 50.0,        # N
            'safety_margin': 0.02,      # 2cm
        }
        
    def execute_pick_and_place(self, target_class: str, place_position: np.ndarray):
        """í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹¤í–‰"""
        print(f"ğŸ¯ í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œì‘: {target_class}")
        
        try:
            # 1. ë¬¼ì²´ ê°ì§€
            self.state = PickPlaceState.DETECTING
            detected_object = self.detect_object(target_class)
            
            if detected_object is None:
                raise Exception(f"ë¬¼ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {target_class}")
            
            self.current_object = detected_object
            print(f"âœ“ ë¬¼ì²´ ê°ì§€ ì™„ë£Œ: {detected_object.class_name} "
                  f"(ì‹ ë¢°ë„: {detected_object.confidence:.2f})")
            
            # 2. ê·¸ë˜ìŠ¤í•‘ ê³„íš
            self.state = PickPlaceState.PLANNING
            grasp_pose = self.plan_grasp(detected_object)
            self.current_grasp = grasp_pose
            print(f"âœ“ ê·¸ë˜ìŠ¤í•‘ ê³„íš ì™„ë£Œ (í’ˆì§ˆ: {grasp_pose.quality_score:.2f})")
            
            # 3. í”½ ì‹¤í–‰
            self.execute_pick(grasp_pose)
            print("âœ“ í”½ ì™„ë£Œ")
            
            # 4. í”Œë ˆì´ìŠ¤ ì‹¤í–‰
            self.execute_place(place_position)
            print("âœ“ í”Œë ˆì´ìŠ¤ ì™„ë£Œ")
            
            self.state = PickPlaceState.COMPLETED
            print("ğŸ‰ í”½ì•¤í”Œë ˆì´ìŠ¤ ì„±ê³µ!")
            
            return True
            
        except Exception as e:
            self.state = PickPlaceState.FAILED
            print(f"âŒ ì‹¤íŒ¨: {e}")
            self.robot.stop()
            return False
    
    def detect_object(self, target_class: str) -> Optional[DetectedObject]:
        """ë¬¼ì²´ ê°ì§€"""
        # êµ¬í˜„ì€ ì„¹ì…˜ 2ì—ì„œ
        pass
    
    def plan_grasp(self, obj: DetectedObject) -> GraspPose:
        """ê·¸ë˜ìŠ¤í•‘ ê³„íš"""
        # êµ¬í˜„ì€ ì„¹ì…˜ 3ì—ì„œ
        pass
    
    def execute_pick(self, grasp_pose: GraspPose):
        """í”½ ì‹¤í–‰"""
        # êµ¬í˜„ì€ ì„¹ì…˜ 4ì—ì„œ
        pass
    
    def execute_place(self, position: np.ndarray):
        """í”Œë ˆì´ìŠ¤ ì‹¤í–‰"""
        # êµ¬í˜„ì€ ì„¹ì…˜ 5ì—ì„œ
        pass
```

---

## 2. ë¬¼ì²´ ê°ì§€ ë° 3D í¬ì¦ˆ ì¶”ì •

### 2.1 YOLO ê¸°ë°˜ ë¬¼ì²´ ê°ì§€

**YOLOv8 í†µí•©**
```python
from ultralytics import YOLO
import torch

class ObjectDetector:
    def __init__(self):
        # YOLOv8 ëª¨ë¸ ë¡œë“œ
        self.model = YOLO('yolov8n.pt')  # nano ë²„ì „
        
        # ê´€ì‹¬ í´ë˜ìŠ¤
        self.target_classes = [
            'bottle', 'cup', 'bowl', 'banana', 'apple',
            'orange', 'book', 'cell phone', 'keyboard', 'mouse'
        ]
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’
        self.confidence_threshold = 0.5
        
    def detect(self, rgb_image: np.ndarray) -> List[dict]:
        """ë¬¼ì²´ ê°ì§€"""
        # YOLO ì¶”ë¡ 
        results = self.model(rgb_image, verbose=False)
        
        detections = []
        
        for result in results:
            boxes = result.boxes
            
            for i, box in enumerate(boxes):
                # í´ë˜ìŠ¤ ë° ì‹ ë¢°ë„
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                class_name = self.model.names[class_id]
                
                # í•„í„°ë§
                if confidence < self.confidence_threshold:
                    continue
                if class_name not in self.target_classes:
                    continue
                
                # ë°”ìš´ë”© ë°•ìŠ¤
                xyxy = box.xyxy[0].cpu().numpy()
                
                detection = {
                    'class_name': class_name,
                    'confidence': confidence,
                    'bbox': xyxy,  # [x1, y1, x2, y2]
                }
                
                detections.append(detection)
        
        return detections
    
    def visualize(self, image: np.ndarray, detections: List[dict]) -> np.ndarray:
        """ê°ì§€ ê²°ê³¼ ì‹œê°í™”"""
        vis_image = image.copy()
        
        for det in detections:
            x1, y1, x2, y2 = det['bbox'].astype(int)
            
            # ë°”ìš´ë”© ë°•ìŠ¤
            cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ë ˆì´ë¸”
            label = f"{det['class_name']} {det['confidence']:.2f}"
            cv2.putText(vis_image, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        return vis_image
```

### 2.2 3D í¬ì¦ˆ ì¶”ì •

**RGB-D ê¸°ë°˜ 3D ìœ„ì¹˜ ì¶”ì •**
```python
class PoseEstimator:
    def __init__(self, camera_intrinsics):
        """
        camera_intrinsics: 3x3 ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° í–‰ë ¬
        """
        self.K = camera_intrinsics
        self.fx = camera_intrinsics[0, 0]
        self.fy = camera_intrinsics[1, 1]
        self.cx = camera_intrinsics[0, 2]
        self.cy = camera_intrinsics[1, 2]
        
    def estimate_3d_pose(self, detection: dict, depth_image: np.ndarray) -> DetectedObject:
        """2D ê°ì§€ + ê¹Šì´ â†’ 3D í¬ì¦ˆ"""
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬
        x1, y1, x2, y2 = detection['bbox']
        cx_2d = (x1 + x2) / 2
        cy_2d = (y1 + y2) / 2
        
        # ê¹Šì´ ê°’ ì¶”ì¶œ (ì¤‘ì•™ ì˜ì—­ì˜ ì¤‘ê°„ê°’)
        bbox_depth = depth_image[int(y1):int(y2), int(x1):int(x2)]
        
        # ì´ìƒì¹˜ ì œê±° í›„ ì¤‘ê°„ê°’ ì‚¬ìš©
        valid_depths = bbox_depth[bbox_depth > 0]
        if len(valid_depths) == 0:
            raise ValueError("ê¹Šì´ ë°ì´í„° ì—†ìŒ")
        
        depth = np.median(valid_depths)
        
        # 2D â†’ 3D ë³€í™˜ (ì¹´ë©”ë¼ ì¢Œí‘œê³„)
        x_cam = (cx_2d - self.cx) * depth / self.fx
        y_cam = (cy_2d - self.cy) * depth / self.fy
        z_cam = depth
        
        position_camera = np.array([x_cam, y_cam, z_cam])
        
        # ì¹´ë©”ë¼ â†’ ë¡œë´‡ ë² ì´ìŠ¤ ì¢Œí‘œ ë³€í™˜
        position_base = self.camera_to_base(position_camera)
        
        # ë¬¼ì²´ í¬ê¸° ì¶”ì •
        width_pixels = x2 - x1
        height_pixels = y2 - y1
        
        width_meters = width_pixels * depth / self.fx
        height_meters = height_pixels * depth / self.fy
        
        # ëŒ€ëµì ì¸ ê¹Šì´ (ë¬¼ì²´ ì¢…ë¥˜ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
        depth_meters = min(width_meters, height_meters) * 0.8
        
        dimensions = np.array([width_meters, height_meters, depth_meters])
        
        # ê¸°ë³¸ ë°©í–¥ (ìœ„ì—ì„œ ì¡ê¸°)
        orientation = np.eye(3)
        
        # ê·¸ë˜ìŠ¤í•‘ í¬ì¸íŠ¸ ìƒì„±
        grasp_points = self.generate_grasp_points(
            position_base, orientation, dimensions
        )
        
        return DetectedObject(
            class_name=detection['class_name'],
            confidence=detection['confidence'],
            bbox_2d=detection['bbox'],
            position_3d=position_base,
            orientation_3d=orientation,
            dimensions=dimensions,
            grasp_points=grasp_points
        )
    
    def camera_to_base(self, pos_camera: np.ndarray) -> np.ndarray:
        """ì¹´ë©”ë¼ ì¢Œí‘œ â†’ ë¡œë´‡ ë² ì´ìŠ¤ ì¢Œí‘œ"""
        # ì¹´ë©”ë¼ ìœ„ì¹˜ (ë¡œë´‡ ë² ì´ìŠ¤ ê¸°ì¤€)
        camera_position = np.array([0.0, 0.0, 0.5])  # ì˜ˆì‹œ
        
        # ì¹´ë©”ë¼ íšŒì „ (ì•„ë˜ë¥¼ í–¥í•¨)
        camera_rotation = np.array([
            [1, 0, 0],
            [0, -1, 0],
            [0, 0, -1]
        ])
        
        # ë³€í™˜
        pos_base = camera_rotation @ pos_camera + camera_position
        
        return pos_base
    
    def generate_grasp_points(self, position: np.ndarray, 
                             orientation: np.ndarray,
                             dimensions: np.ndarray) -> List[np.ndarray]:
        """ê°€ëŠ¥í•œ ê·¸ë˜ìŠ¤í•‘ í¬ì¸íŠ¸ ìƒì„±"""
        grasp_points = []
        
        # ìœ„ì—ì„œ ì¡ê¸°
        top_grasp = position.copy()
        top_grasp[2] += dimensions[2] / 2
        grasp_points.append(top_grasp)
        
        # ì¸¡ë©´ì—ì„œ ì¡ê¸° (ì–‘ìª½)
        side_offset = dimensions[0] / 2
        for sign in [-1, 1]:
            side_grasp = position.copy()
            side_grasp[0] += sign * side_offset
            grasp_points.append(side_grasp)
        
        return grasp_points
    
    def visualize_3d(self, rgb_image: np.ndarray, obj: DetectedObject) -> np.ndarray:
        """3D ì •ë³´ ì˜¤ë²„ë ˆì´"""
        vis = rgb_image.copy()
        
        # ë°”ìš´ë”© ë°•ìŠ¤
        x1, y1, x2, y2 = obj.bbox_2d.astype(int)
        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # 3D ì •ë³´ í…ìŠ¤íŠ¸
        pos = obj.position_3d
        info_text = [
            f"Class: {obj.class_name}",
            f"Pos: ({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})m",
            f"Size: {obj.dimensions[0]:.2f}x{obj.dimensions[1]:.2f}x{obj.dimensions[2]:.2f}m"
        ]
        
        y_offset = y1 - 40
        for text in info_text:
            cv2.putText(vis, text, (x1, y_offset),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
            y_offset += 15
        
        # ê·¸ë˜ìŠ¤í•‘ í¬ì¸íŠ¸ í‘œì‹œ (ì¹´ë©”ë¼ íˆ¬ì˜)
        for gp in obj.grasp_points:
            # 3D â†’ 2D íˆ¬ì˜
            gp_2d = self.project_to_2d(gp)
            if gp_2d is not None:
                gp_x, gp_y = gp_2d.astype(int)
                cv2.circle(vis, (gp_x, gp_y), 5, (0, 0, 255), -1)
        
        return vis
    
    def project_to_2d(self, point_3d: np.ndarray) -> Optional[np.ndarray]:
        """3D í¬ì¸íŠ¸ â†’ 2D ì´ë¯¸ì§€ íˆ¬ì˜"""
        # ë² ì´ìŠ¤ â†’ ì¹´ë©”ë¼ ì¢Œí‘œ ë³€í™˜
        point_cam = self.base_to_camera(point_3d)
        
        if point_cam[2] <= 0:
            return None
        
        # íˆ¬ì˜
        x = self.fx * point_cam[0] / point_cam[2] + self.cx
        y = self.fy * point_cam[1] / point_cam[2] + self.cy
        
        return np.array([x, y])
    
    def base_to_camera(self, pos_base: np.ndarray) -> np.ndarray:
        """ë¡œë´‡ ë² ì´ìŠ¤ â†’ ì¹´ë©”ë¼ ì¢Œí‘œ"""
        # camera_to_baseì˜ ì—­ë³€í™˜
        camera_position = np.array([0.0, 0.0, 0.5])
        camera_rotation = np.array([
            [1, 0, 0],
            [0, -1, 0],
            [0, 0, -1]
        ])
        
        pos_cam = camera_rotation.T @ (pos_base - camera_position)
        return pos_cam
```

---

## 3. ê·¸ë˜ìŠ¤í•‘ ê³„íš

### 3.1 ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ ê³„ì‚°

**Grasp Planner**
```python
from scipy.spatial.transform import Rotation as R

class GraspPlanner:
    def __init__(self):
        self.gripper_width_range = (0.0, 0.08)  # 0-8cm
        
    def plan_grasp(self, obj: DetectedObject) -> GraspPose:
        """ìµœì  ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ ê³„íš"""
        
        # ëª¨ë“  ê°€ëŠ¥í•œ ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ ìƒì„±
        candidate_grasps = []
        
        for grasp_point in obj.grasp_points:
            # ì—¬ëŸ¬ ì ‘ê·¼ ê°ë„ ì‹œë„
            for approach_angle in [0, 45, 90, 135, 180]:
                grasp = self.create_grasp_pose(
                    grasp_point, 
                    obj.orientation_3d,
                    approach_angle,
                    obj.dimensions
                )
                
                if grasp is not None:
                    # ê·¸ë˜ìŠ¤í•‘ í’ˆì§ˆ í‰ê°€
                    grasp.quality_score = self.evaluate_grasp(grasp, obj)
                    candidate_grasps.append(grasp)
        
        if not candidate_grasps:
            raise Exception("ì‹¤í–‰ ê°€ëŠ¥í•œ ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ ì—†ìŒ")
        
        # ìµœê³  í’ˆì§ˆì˜ ê·¸ë˜ìŠ¤í•‘ ì„ íƒ
        best_grasp = max(candidate_grasps, key=lambda g: g.quality_score)
        
        return best_grasp
    
    def create_grasp_pose(self, position: np.ndarray, 
                         obj_orientation: np.ndarray,
                         approach_angle_deg: float,
                         dimensions: np.ndarray) -> Optional[GraspPose]:
        """ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ ìƒì„±"""
        
        # ì ‘ê·¼ ë°©í–¥ (Zì¶• ê¸°ì¤€ íšŒì „)
        approach_angle_rad = np.deg2rad(approach_angle_deg)
        
        # ê¸°ë³¸: ìœ„ì—ì„œ ì•„ë˜ë¡œ ì ‘ê·¼
        approach_vector = np.array([0, 0, -1])
        
        # íšŒì „ ì ìš©
        rotation = R.from_euler('z', approach_angle_rad)
        approach_vector = rotation.apply(approach_vector)
        
        # ê·¸ë¦¬í¼ ë°©í–¥ (ì ‘ê·¼ ë°©í–¥ì— ìˆ˜ì§)
        gripper_orientation = self.compute_gripper_orientation(approach_vector)
        
        # ê·¸ë˜ìŠ¤í•‘ í­ ê³„ì‚°
        grasp_width = min(dimensions[0], dimensions[1])
        
        # ê·¸ë¦¬í¼ ë²”ìœ„ ì²´í¬
        if not (self.gripper_width_range[0] <= grasp_width <= self.gripper_width_range[1]):
            return None
        
        return GraspPose(
            position=position.copy(),
            orientation=gripper_orientation,
            approach_vector=approach_vector,
            grasp_width=grasp_width,
            quality_score=0.0  # ë‚˜ì¤‘ì— ê³„ì‚°
        )
    
    def compute_gripper_orientation(self, approach_vector: np.ndarray) -> np.ndarray:
        """ì ‘ê·¼ ë²¡í„°ë¡œë¶€í„° ê·¸ë¦¬í¼ ë°©í–¥ ê³„ì‚°"""
        
        # Zì¶•ì´ ì ‘ê·¼ ë°©í–¥ì„ í–¥í•˜ë„ë¡
        z_axis = -approach_vector / np.linalg.norm(approach_vector)
        
        # Xì¶• (ì„ì˜ ì„ íƒ, ì¤‘ë ¥ ë°©í–¥ì— ìˆ˜ì§)
        if abs(z_axis[2]) < 0.9:
            x_axis = np.cross([0, 0, 1], z_axis)
        else:
            x_axis = np.cross([1, 0, 0], z_axis)
        x_axis = x_axis / np.linalg.norm(x_axis)
        
        # Yì¶• (ì˜¤ë¥¸ì† ë²•ì¹™)
        y_axis = np.cross(z_axis, x_axis)
        
        # íšŒì „ í–‰ë ¬
        rotation_matrix = np.column_stack([x_axis, y_axis, z_axis])
        
        return rotation_matrix
    
    def evaluate_grasp(self, grasp: GraspPose, obj: DetectedObject) -> float:
        """ê·¸ë˜ìŠ¤í•‘ í’ˆì§ˆ í‰ê°€ (0~1)"""
        
        score = 0.0
        
        # 1. ì ‘ê·¼ ê°€ëŠ¥ì„± (ìœ„ì—ì„œ ì ‘ê·¼ì´ ë” ì¢‹ìŒ)
        approach_score = max(0, grasp.approach_vector[2])  # Z ì„±ë¶„
        score += 0.3 * approach_score
        
        # 2. ê·¸ë˜ìŠ¤í•‘ í­ (ì¤‘ê°„ í¬ê¸°ê°€ ì•ˆì •ì )
        optimal_width = (self.gripper_width_range[0] + self.gripper_width_range[1]) / 2
        width_score = 1.0 - abs(grasp.grasp_width - optimal_width) / optimal_width
        score += 0.3 * width_score
        
        # 3. ë¬¼ì²´ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ (ì¤‘ì‹¬ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
        distance = np.linalg.norm(grasp.position - obj.position_3d)
        distance_score = np.exp(-distance * 10)
        score += 0.2 * distance_score
        
        # 4. ì•ˆì •ì„± (í…Œì´ë¸” ìœ„ì˜ ë¬¼ì²´ëŠ” ìœ„ì—ì„œ ì¡ê¸°)
        if obj.position_3d[2] > 0:  # í…Œì´ë¸” ìœ„
            if grasp.approach_vector[2] < -0.5:  # ìœ„ì—ì„œ ì ‘ê·¼
                score += 0.2
        
        return np.clip(score, 0, 1)
```

---

## 4. ëª¨ì…˜ í”Œë˜ë‹ ë° ê¶¤ì  ìƒì„±

### 4.1 RRT ê¸°ë°˜ ê²½ë¡œ ê³„íš

**RRT Path Planner**
```python
import random
from scipy.spatial import KDTree

class MotionPlanner:
    def __init__(self, robot):
        self.robot = robot
        
        # ê´€ì ˆ í•œê³„
        self.joint_limits = robot.get_joint_limits()
        
        # RRT íŒŒë¼ë¯¸í„°
        self.rrt_max_iterations = 1000
        self.rrt_step_size = 0.1  # rad
        self.rrt_goal_bias = 0.1
        
    def plan_to_pose(self, target_pos: np.ndarray, 
                    target_rot: np.ndarray) -> Optional[Trajectory]:
        """ëª©í‘œ í¬ì¦ˆë¡œ ê²½ë¡œ ê³„íš"""
        
        # ì—­ê¸°êµ¬í•™ìœ¼ë¡œ ëª©í‘œ ê´€ì ˆ ê°ë„ ê³„ì‚°
        target_joints = self.robot.inverse_kinematics(target_pos, target_rot)
        
        if target_joints is None:
            print("ì—­ê¸°êµ¬í•™ ì‹¤íŒ¨")
            return None
        
        # í˜„ì¬ ê´€ì ˆ ê°ë„
        start_joints = self.robot.get_joint_angles()
        
        # RRTë¡œ ê²½ë¡œ ê³„íš
        path = self.rrt_plan(start_joints, target_joints)
        
        if path is None:
            print("ê²½ë¡œ ê³„íš ì‹¤íŒ¨")
            return None
        
        # ê¶¤ì  ìƒì„± (ì‹œê°„ íŒŒë¼ë¯¸í„°í™”)
        trajectory = self.create_trajectory(path)
        
        return trajectory
    
    def rrt_plan(self, start: np.ndarray, goal: np.ndarray) -> Optional[List[np.ndarray]]:
        """RRT ê²½ë¡œ ê³„íš"""
        
        # íŠ¸ë¦¬ ì´ˆê¸°í™”
        tree = [start]
        parent = {0: None}
        
        for iteration in range(self.rrt_max_iterations):
            # ëœë¤ ìƒ˜í”Œ (goal bias ì ìš©)
            if random.random() < self.rrt_goal_bias:
                sample = goal
            else:
                sample = self.random_configuration()
            
            # ê°€ì¥ ê°€ê¹Œìš´ ë…¸ë“œ ì°¾ê¸°
            nearest_idx = self.nearest_node(tree, sample)
            nearest = tree[nearest_idx]
            
            # ìƒˆ ë…¸ë“œë¡œ í™•ì¥
            new_node = self.steer(nearest, sample)
            
            # ì¶©ëŒ ì²´í¬
            if self.is_collision_free(nearest, new_node):
                # íŠ¸ë¦¬ì— ì¶”ê°€
                new_idx = len(tree)
                tree.append(new_node)
                parent[new_idx] = nearest_idx
                
                # ëª©í‘œ ë„ë‹¬ í™•ì¸
                if np.linalg.norm(new_node - goal) < self.rrt_step_size:
                    # ê²½ë¡œ ì¶”ì¶œ
                    path = self.extract_path(tree, parent, new_idx)
                    path.append(goal)
                    return path
        
        print(f"RRT: {self.rrt_max_iterations}íšŒ ë°˜ë³µ í›„ ì‹¤íŒ¨")
        return None
    
    def random_configuration(self) -> np.ndarray:
        """ëœë¤ ê´€ì ˆ ì„¤ì •"""
        config = np.random.uniform(
            self.joint_limits[:, 0],
            self.joint_limits[:, 1]
        )
        return config
    
    def nearest_node(self, tree: List[np.ndarray], sample: np.ndarray) -> int:
        """ê°€ì¥ ê°€ê¹Œìš´ ë…¸ë“œ ì°¾ê¸°"""
        tree_array = np.array(tree)
        distances = np.linalg.norm(tree_array - sample, axis=1)
        return np.argmin(distances)
    
    def steer(self, from_node: np.ndarray, to_node: np.ndarray) -> np.ndarray:
        """ë…¸ë“œ í™•ì¥"""
        direction = to_node - from_node
        distance = np.linalg.norm(direction)
        
        if distance < self.rrt_step_size:
            return to_node
        
        direction = direction / distance
        new_node = from_node + direction * self.rrt_step_size
        
        # ê´€ì ˆ í•œê³„ í´ë¦¬í•‘
        new_node = np.clip(new_node, 
                          self.joint_limits[:, 0],
                          self.joint_limits[:, 1])
        
        return new_node
    
    def is_collision_free(self, config1: np.ndarray, config2: np.ndarray) -> bool:
        """ì¶©ëŒ ì²´í¬ (ê°„ë‹¨í•œ ë²„ì „)"""
        
        # ì¤‘ê°„ ì§€ì ë“¤ ì²´í¬
        steps = 10
        for i in range(steps + 1):
            t = i / steps
            config = config1 + t * (config2 - config1)
            
            # ìì²´ ì¶©ëŒ ì²´í¬
            if self.is_self_collision(config):
                return False
            
            # í™˜ê²½ ì¶©ëŒ ì²´í¬
            if self.is_environment_collision(config):
                return False
        
        return True
    
    def is_self_collision(self, config: np.ndarray) -> bool:
        """ìì²´ ì¶©ëŒ ì²´í¬"""
        # ê°„ë‹¨í•œ ë²„ì „: ê´€ì ˆ í•œê³„ë§Œ ì²´í¬
        return np.any(config < self.joint_limits[:, 0]) or \
               np.any(config > self.joint_limits[:, 1])
    
    def is_environment_collision(self, config: np.ndarray) -> bool:
        """í™˜ê²½ ì¶©ëŒ ì²´í¬"""
        # ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ì²´í¬
        # (ì‹¤ì œë¡œëŠ” MuJoCo collision detection ì‚¬ìš©)
        return False
    
    def extract_path(self, tree: List[np.ndarray], 
                    parent: dict, node_idx: int) -> List[np.ndarray]:
        """ê²½ë¡œ ì¶”ì¶œ"""
        path = []
        current = node_idx
        
        while current is not None:
            path.append(tree[current])
            current = parent[current]
        
        path.reverse()
        return path
    
    def create_trajectory(self, path: List[np.ndarray], 
                         max_velocity: float = 1.0,
                         max_acceleration: float = 2.0) -> Trajectory:
        """ê²½ë¡œë¥¼ ì‹œê°„ íŒŒë¼ë¯¸í„°í™”ëœ ê¶¤ì ìœ¼ë¡œ ë³€í™˜"""
        
        if len(path) < 2:
            raise ValueError("ê²½ë¡œê°€ ë„ˆë¬´ ì§§ìŒ")
        
        waypoints = []
        timestamps = [0.0]
        velocities = []
        accelerations = []
        
        # ì²« ì›¨ì´í¬ì¸íŠ¸
        waypoints.append(path[0])
        velocities.append(np.zeros_like(path[0]))
        accelerations.append(np.zeros_like(path[0]))
        
        current_time = 0.0
        
        # ê° ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ ì‹œê°„ ê³„ì‚°
        for i in range(1, len(path)):
            prev = path[i - 1]
            current = path[i]
            
            # ê±°ë¦¬
            distance = np.linalg.norm(current - prev)
            
            # ì‹œê°„ (ì‚¼ê°í˜• ì†ë„ í”„ë¡œíŒŒì¼)
            segment_time = 2 * distance / max_velocity
            current_time += segment_time
            
            waypoints.append(current)
            timestamps.append(current_time)
            
            # ì†ë„ (ê°„ë‹¨í•œ ì¶”ì •)
            velocity = (current - prev) / segment_time
            velocities.append(velocity)
            
            # ê°€ì†ë„ (ê°„ë‹¨í•œ ì¶”ì •)
            if i > 1:
                accel = (velocities[-1] - velocities[-2]) / segment_time
            else:
                accel = velocities[-1] / segment_time
            accelerations.append(accel)
        
        return Trajectory(
            waypoints=waypoints,
            timestamps=timestamps,
            velocities=velocities,
            accelerations=accelerations
        )
```

---

## 5. í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹¤í–‰

### 5.1 í”½ ì‹œí€€ìŠ¤

**Pick Execution**
```python
def execute_pick(self, grasp_pose: GraspPose):
    """í”½ ì‹œí€€ìŠ¤ ì‹¤í–‰"""
    
    print("ğŸ“ Step 1: ì ‘ê·¼ ìœ„ì¹˜ë¡œ ì´ë™")
    self.state = PickPlaceState.APPROACHING
    
    # ì ‘ê·¼ ìœ„ì¹˜ (ê·¸ë˜ìŠ¤í•‘ ìœ„ì¹˜ì—ì„œ ì•½ê°„ ë–¨ì–´ì§„ ê³³)
    approach_pos = grasp_pose.position + \
                   grasp_pose.approach_vector * self.config['approach_distance']
    
    # ì ‘ê·¼ ìœ„ì¹˜ë¡œ ê²½ë¡œ ê³„íš ë° ì´ë™
    approach_traj = self.motion_planner.plan_to_pose(
        approach_pos, 
        grasp_pose.orientation
    )
    
    if approach_traj is None:
        raise Exception("ì ‘ê·¼ ê²½ë¡œ ê³„íš ì‹¤íŒ¨")
    
    self.execute_trajectory(approach_traj)
    print("  âœ“ ì ‘ê·¼ ìœ„ì¹˜ ë„ë‹¬")
    
    # ê·¸ë¦¬í¼ ì—´ê¸°
    self.robot.set_gripper_position(0.0)
    time.sleep(0.5)
    
    print("ğŸ“ Step 2: ë¬¼ì²´ë¡œ ì´ë™")
    # ì§ì„  ì´ë™ (ê·¸ë˜ìŠ¤í•‘ ìœ„ì¹˜ë¡œ)
    self.move_linear(grasp_pose.position, grasp_pose.orientation)
    print("  âœ“ ê·¸ë˜ìŠ¤í•‘ ìœ„ì¹˜ ë„ë‹¬")
    
    print("ğŸ“ Step 3: ê·¸ë¦¬í¼ ë‹«ê¸°")
    self.state = PickPlaceState.GRASPING
    
    # ê·¸ë¦¬í¼ ë‹«ê¸°
    target_gripper_pos = grasp_pose.grasp_width / self.robot.max_gripper_width
    self.robot.set_gripper_position(target_gripper_pos)
    time.sleep(1.0)
    
    # ê·¸ë˜ìŠ¤í•‘ ì„±ê³µ í™•ì¸
    if not self.verify_grasp():
        raise Exception("ê·¸ë˜ìŠ¤í•‘ ì‹¤íŒ¨")
    
    print("  âœ“ ë¬¼ì²´ ì¡ìŒ")
    
    print("ğŸ“ Step 4: ë¦¬í”„íŠ¸")
    self.state = PickPlaceState.LIFTING
    
    # ìœ„ë¡œ ë“¤ì–´ì˜¬ë¦¬ê¸°
    lift_pos = grasp_pose.position.copy()
    lift_pos[2] += self.config['lift_height']
    
    self.move_linear(lift_pos, grasp_pose.orientation)
    print("  âœ“ ë¦¬í”„íŠ¸ ì™„ë£Œ")

def verify_grasp(self) -> bool:
    """ê·¸ë˜ìŠ¤í•‘ ì„±ê³µ í™•ì¸"""
    
    # ê·¸ë¦¬í¼ ìœ„ì¹˜ í™•ì¸
    gripper_pos = self.robot.get_gripper_position()
    
    # ì™„ì „íˆ ë‹«íˆì§€ ì•Šì•˜ìœ¼ë©´ ë¬¼ì²´ë¥¼ ì¡ì€ ê²ƒ
    if gripper_pos > 0.1:  # 10% ì´ìƒ ì—´ë ¤ìˆìŒ
        return True
    
    # ì¶”ê°€: í˜ ì„¼ì„œê°€ ìˆë‹¤ë©´ ì ‘ì´‰ë ¥ í™•ì¸
    # contact_force = self.robot.get_gripper_force()
    # if contact_force > threshold:
    #     return True
    
    return False

def move_linear(self, target_pos: np.ndarray, target_rot: np.ndarray, 
                duration: float = 2.0):
    """ì§ì„  ê²½ë¡œë¡œ ì´ë™"""
    
    current_pos, current_rot = self.robot.get_end_effector_pose()
    
    steps = int(duration * 60)  # 60 Hz
    
    for i in range(steps):
        t = (i + 1) / steps
        
        # ìœ„ì¹˜ ë³´ê°„
        interp_pos = current_pos + t * (target_pos - current_pos)
        
        # íšŒì „ ë³´ê°„ (SLERP)
        from scipy.spatial.transform import Rotation as R
        rot_current = R.from_matrix(current_rot)
        rot_target = R.from_matrix(target_rot)
        
        rot_interp = rot_current.slerp(rot_target, t)
        interp_rot = rot_interp.as_matrix()
        
        # ì—­ê¸°êµ¬í•™
        joint_angles = self.robot.inverse_kinematics(interp_pos, interp_rot)
        
        if joint_angles is not None:
            self.robot.set_joint_angles(joint_angles)
        
        time.sleep(1.0 / 60.0)

def execute_trajectory(self, trajectory: Trajectory):
    """ê¶¤ì  ì‹¤í–‰"""
    
    start_time = time.time()
    
    for i, (waypoint, timestamp) in enumerate(zip(trajectory.waypoints, 
                                                   trajectory.timestamps)):
        # íƒ€ì´ë° ë§ì¶”ê¸°
        while time.time() - start_time < timestamp:
            time.sleep(0.001)
        
        # ê´€ì ˆ ê°ë„ ì„¤ì •
        self.robot.set_joint_angles(waypoint)
```

### 5.2 í”Œë ˆì´ìŠ¤ ì‹œí€€ìŠ¤

**Place Execution**
```python
def execute_place(self, place_position: np.ndarray):
    """í”Œë ˆì´ìŠ¤ ì‹œí€€ìŠ¤ ì‹¤í–‰"""
    
    print("ğŸ“ Step 5: í”Œë ˆì´ìŠ¤ ìœ„ì¹˜ë¡œ ì´ë™")
    self.state = PickPlaceState.TRANSFERRING
    
    # í”Œë ˆì´ìŠ¤ ìœ„ì¹˜ ìœ„ë¡œ ì´ë™
    transfer_pos = place_position.copy()
    transfer_pos[2] += self.config['lift_height']
    
    # ë°©í–¥ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
    place_orientation = np.eye(3)
    
    # ê²½ë¡œ ê³„íš ë° ì‹¤í–‰
    transfer_traj = self.motion_planner.plan_to_pose(
        transfer_pos,
        place_orientation
    )
    
    if transfer_traj is None:
        raise Exception("ì´ë™ ê²½ë¡œ ê³„íš ì‹¤íŒ¨")
    
    self.execute_trajectory(transfer_traj)
    print("  âœ“ í”Œë ˆì´ìŠ¤ ìœ„ì¹˜ ìƒê³µ ë„ë‹¬")
    
    print("ğŸ“ Step 6: í•˜ê°•")
    self.state = PickPlaceState.PLACING
    
    # ì²œì²œíˆ í•˜ê°•
    self.move_linear(place_position, place_orientation, duration=2.0)
    print("  âœ“ í”Œë ˆì´ìŠ¤ ìœ„ì¹˜ ë„ë‹¬")
    
    print("ğŸ“ Step 7: ë¬¼ì²´ ë†“ê¸°")
    # ê·¸ë¦¬í¼ ì—´ê¸°
    self.robot.set_gripper_position(0.0)
    time.sleep(1.0)
    print("  âœ“ ë¬¼ì²´ ë†“ìŒ")
    
    print("ğŸ“ Step 8: ë³µê·€")
    self.state = PickPlaceState.RETREATING
    
    # ìœ„ë¡œ ë³µê·€
    retreat_pos = place_position.copy()
    retreat_pos[2] += self.config['lift_height']
    
    self.move_linear(retreat_pos, place_orientation, duration=1.5)
    print("  âœ“ ë³µê·€ ì™„ë£Œ")
```

---

## 6. í†µí•© ì˜ˆì œ

### 6.1 ì™„ì „í•œ í”½ì•¤í”Œë ˆì´ìŠ¤ ë°ëª¨

**ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸**
```python
import time
import numpy as np
import cv2

def main():
    """í”½ì•¤í”Œë ˆì´ìŠ¤ ë°ëª¨"""
    
    print("=" * 60)
    print("  XLeRobot í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # 1. ë¡œë´‡ ë° ì¹´ë©”ë¼ ì´ˆê¸°í™”
    print("\nğŸ¤– ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
    robot = XLeRobotMuJoCo()
    camera = RGBDCamera()
    
    # 2. í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œìŠ¤í…œ ìƒì„±
    system = PickAndPlaceSystem(robot, camera)
    
    # 3. í™ˆ í¬ì§€ì…˜ìœ¼ë¡œ ì´ë™
    print("ğŸ  í™ˆ í¬ì§€ì…˜ìœ¼ë¡œ ì´ë™")
    robot.go_home()
    time.sleep(2)
    
    # 4. ë¬¼ì²´ ê°ì§€
    print("\nğŸ“· ë¬¼ì²´ ê°ì§€ ì¤‘...")
    rgb_image = camera.get_rgb()
    depth_image = camera.get_depth()
    
    # YOLO ê°ì§€
    detections = system.detector.detect(rgb_image)
    print(f"  ë°œê²¬ëœ ë¬¼ì²´: {len(detections)}ê°œ")
    
    for det in detections:
        print(f"    - {det['class_name']} (ì‹ ë¢°ë„: {det['confidence']:.2f})")
    
    # ì‹œê°í™”
    vis_image = system.detector.visualize(rgb_image, detections)
    cv2.imshow("Detections", vis_image)
    cv2.waitKey(2000)
    
    # 5. ì²« ë²ˆì§¸ ë¬¼ì²´ ì„ íƒ
    if not detections:
        print("âŒ ë¬¼ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return
    
    target_detection = detections[0]
    print(f"\nğŸ¯ íƒ€ê²Ÿ: {target_detection['class_name']}")
    
    # 6. 3D í¬ì¦ˆ ì¶”ì •
    print("ğŸ“ 3D í¬ì¦ˆ ì¶”ì • ì¤‘...")
    camera_intrinsics = camera.get_intrinsics()
    pose_estimator = PoseEstimator(camera_intrinsics)
    
    detected_object = pose_estimator.estimate_3d_pose(
        target_detection, 
        depth_image
    )
    
    print(f"  ìœ„ì¹˜: {detected_object.position_3d}")
    print(f"  í¬ê¸°: {detected_object.dimensions}")
    
    # 7. í”Œë ˆì´ìŠ¤ ìœ„ì¹˜ ì„¤ì •
    place_position = np.array([0.4, -0.3, 0.05])  # í…Œì´ë¸”ì˜ ë‹¤ë¥¸ ìœ„ì¹˜
    
    print(f"\nğŸ“ í”Œë ˆì´ìŠ¤ ìœ„ì¹˜: {place_position}")
    
    # 8. í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹¤í–‰
    print("\nğŸš€ í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œì‘!\n")
    
    success = system.execute_pick_and_place(
        target_detection['class_name'],
        place_position
    )
    
    if success:
        print("\n" + "=" * 60)
        print("  âœ… í”½ì•¤í”Œë ˆì´ìŠ¤ ì„±ê³µ!")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("  âŒ í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹¤íŒ¨")
        print("=" * 60)
    
    # 9. í™ˆìœ¼ë¡œ ë³µê·€
    print("\nğŸ  í™ˆ í¬ì§€ì…˜ìœ¼ë¡œ ë³µê·€")
    robot.go_home()
    
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
```

### 6.2 ì—°ì† í”½ì•¤í”Œë ˆì´ìŠ¤

**ë‹¤ì¤‘ ë¬¼ì²´ ì²˜ë¦¬**
```python
def continuous_pick_and_place():
    """ì—°ì† í”½ì•¤í”Œë ˆì´ìŠ¤ (í…Œì´ë¸” ì •ë¦¬)"""
    
    system = PickAndPlaceSystem(robot, camera)
    
    # ì†ŒìŠ¤ ì˜ì—­ê³¼ íƒ€ê²Ÿ ì˜ì—­ ì •ì˜
    source_area = {
        'x_range': (0.2, 0.5),
        'y_range': (-0.2, 0.2)
    }
    
    target_positions = [
        np.array([0.4, -0.4, 0.05]),
        np.array([0.4, -0.5, 0.05]),
        np.array([0.5, -0.4, 0.05]),
        np.array([0.5, -0.5, 0.05]),
    ]
    
    placed_count = 0
    
    while placed_count < len(target_positions):
        # ë¬¼ì²´ ê°ì§€
        rgb = camera.get_rgb()
        depth = camera.get_depth()
        
        detections = system.detector.detect(rgb)
        
        if not detections:
            print("ë” ì´ìƒ ë¬¼ì²´ê°€ ì—†ìŠµë‹ˆë‹¤")
            break
        
        # ì†ŒìŠ¤ ì˜ì—­ ë‚´ì˜ ë¬¼ì²´ í•„í„°ë§
        valid_objects = []
        
        for det in detections:
            obj = pose_estimator.estimate_3d_pose(det, depth)
            
            if (source_area['x_range'][0] <= obj.position_3d[0] <= source_area['x_range'][1] and
                source_area['y_range'][0] <= obj.position_3d[1] <= source_area['y_range'][1]):
                valid_objects.append((det, obj))
        
        if not valid_objects:
            print("ì†ŒìŠ¤ ì˜ì—­ì— ë¬¼ì²´ê°€ ì—†ìŠµë‹ˆë‹¤")
            break
        
        # ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ ì„ íƒ
        det, obj = min(valid_objects, key=lambda x: np.linalg.norm(x[1].position_3d[:2]))
        
        # í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹¤í–‰
        success = system.execute_pick_and_place(
            det['class_name'],
            target_positions[placed_count]
        )
        
        if success:
            placed_count += 1
            print(f"âœ… {placed_count}/{len(target_positions)} ì™„ë£Œ")
        else:
            print("âŒ ì‹¤íŒ¨, ë‹¤ìŒ ë¬¼ì²´ë¡œ...")
            continue
    
    print(f"\nğŸ‰ ì´ {placed_count}ê°œ ë¬¼ì²´ ì •ë¦¬ ì™„ë£Œ!")
```

---

## 7. ê³ ê¸‰ ê¸°ëŠ¥

### 7.1 í•™ìŠµ ê¸°ë°˜ ê·¸ë˜ìŠ¤í•‘

**ê°„ë‹¨í•œ ê·¸ë˜ìŠ¤í•‘ ë„¤íŠ¸ì›Œí¬**
```python
import torch
import torch.nn as nn

class GraspNet(nn.Module):
    """ê·¸ë˜ìŠ¤í•‘ í’ˆì§ˆ ì˜ˆì¸¡ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self):
        super().__init__()
        
        # ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        self.conv = nn.Sequential(
            nn.Conv2d(4, 32, 3, padding=1),  # RGB + Depth
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)
        )
        
        # ê·¸ë˜ìŠ¤í•‘ í’ˆì§ˆ ì˜ˆì¸¡
        self.fc = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, rgbd_patch):
        """
        rgbd_patch: (B, 4, H, W) - RGB + Depth
        returns: (B, 1) - grasp quality score
        """
        features = self.conv(rgbd_patch)
        features = features.view(features.size(0), -1)
        quality = self.fc(features)
        return quality
```

---

## âœ… í”„ë¡œì íŠ¸ 3 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] YOLO ë¬¼ì²´ ê°ì§€ êµ¬í˜„
- [ ] RGB-D ê¸°ë°˜ 3D í¬ì¦ˆ ì¶”ì •
- [ ] ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ ê³„íš
- [ ] RRT ê²½ë¡œ ê³„íš êµ¬í˜„
- [ ] í”½ ì‹œí€€ìŠ¤ ì‹¤í–‰
- [ ] í”Œë ˆì´ìŠ¤ ì‹œí€€ìŠ¤ ì‹¤í–‰
- [ ] ì™„ì „í•œ í”½ì•¤í”Œë ˆì´ìŠ¤ íŒŒì´í”„ë¼ì¸
- [ ] ì—°ì† í”½ì•¤í”Œë ˆì´ìŠ¤ (ë‹¤ì¤‘ ë¬¼ì²´)

## ğŸ“ í•™ìŠµ ì •ë¦¬

1. **ë¬¼ì²´ ì¸ì‹**: YOLO + ê¹Šì´ ì •ë³´ë¡œ 3D í¬ì¦ˆ ì¶”ì •
2. **ê·¸ë˜ìŠ¤í•‘**: ìµœì  ê·¸ë˜ìŠ¤í•‘ í¬ì¦ˆ ê³„ì‚° ë° í’ˆì§ˆ í‰ê°€
3. **ëª¨ì…˜ í”Œë˜ë‹**: RRT ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¶©ëŒ íšŒí”¼ ê²½ë¡œ ìƒì„±
4. **ì‹¤í–‰ ì œì–´**: ì •ë°€í•œ í”½ì•¤í”Œë ˆì´ìŠ¤ ì‹œí€€ìŠ¤ ì‹¤í–‰
5. **ì‹œìŠ¤í…œ í†µí•©**: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ

---

[â† 8.2 ì»¤ìŠ¤í…€ ì œì–´](02_custom_control.md) | [ë‹¤ìŒ: 8.4 ììœ¨ ë„¤ë¹„ê²Œì´ì…˜ â†’](04_navigation.md)
