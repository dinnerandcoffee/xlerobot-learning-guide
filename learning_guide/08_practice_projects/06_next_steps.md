# 8.6 ë‹¤ìŒ ë‹¨ê³„: ê³ ê¸‰ ì£¼ì œ ë° ì»¤ë®¤ë‹ˆí‹°

XLeRobot í•™ìŠµì„ ë§ˆì¹˜ê³  ë” ë‚˜ì•„ê°€ê¸° ìœ„í•œ ê³ ê¸‰ ì£¼ì œì™€ ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬ ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ğŸ¯ ì´ ì¥ì—ì„œ ë°°ìš¸ ë‚´ìš©

- ê³ ê¸‰ ì—°êµ¬ ì£¼ì œ íƒìƒ‰
- í”„ë¡œì íŠ¸ ê°œì„  ë°©í–¥
- ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ ë°©ë²•
- ì¶”ê°€ í•™ìŠµ ìë£Œ
- ì‹¤ì „ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´

---

## 1. ê³ ê¸‰ ì—°êµ¬ ì£¼ì œ

### 1.1 ê°•í™”í•™ìŠµ ê¸°ë°˜ ì œì–´

**Deep Reinforcement Learning**

XLeRobotì— RL ì ìš©í•˜ê¸°:

```python
import torch
import torch.nn as nn
import numpy as np
from collections import deque
import random

class PPOAgent:
    """PPO ê¸°ë°˜ ë¡œë´‡ ì œì–´ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Actor-Critic ë„¤íŠ¸ì›Œí¬
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()  # í–‰ë™ ë²”ìœ„ [-1, 1]
        )
        
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
        
        self.actor_optimizer = torch.optim.Adam(
            self.actor.parameters(), lr=3e-4
        )
        self.critic_optimizer = torch.optim.Adam(
            self.critic.parameters(), lr=1e-3
        )
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.gamma = 0.99
        self.epsilon = 0.2
        self.buffer = []
    
    def select_action(self, state):
        """í–‰ë™ ì„ íƒ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action = self.actor(state)
        
        # íƒí—˜ì„ ìœ„í•œ ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = torch.randn_like(action) * 0.1
        action = action + noise
        
        return action.squeeze(0).numpy()
    
    def store_transition(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def update(self, batch_size=64):
        """ì •ì±… ì—…ë°ì´íŠ¸"""
        if len(self.buffer) < batch_size:
            return
        
        # ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§
        batch = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([t[0] for t in batch])
        actions = torch.FloatTensor([t[1] for t in batch])
        rewards = torch.FloatTensor([t[2] for t in batch])
        next_states = torch.FloatTensor([t[3] for t in batch])
        dones = torch.FloatTensor([t[4] for t in batch])
        
        # Critic ì—…ë°ì´íŠ¸
        values = self.critic(states).squeeze()
        next_values = self.critic(next_states).squeeze()
        
        target_values = rewards + self.gamma * next_values * (1 - dones)
        critic_loss = nn.MSELoss()(values, target_values.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Actor ì—…ë°ì´íŠ¸
        advantages = (target_values - values).detach()
        
        old_actions = actions
        new_actions = self.actor(states)
        
        # PPO í´ë¦¬í•‘
        ratio = torch.exp(
            -0.5 * ((new_actions - old_actions) ** 2).sum(dim=1)
        )
        
        clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)
        
        actor_loss = -torch.min(
            ratio * advantages,
            clipped_ratio * advantages
        ).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # ë²„í¼ ì •ë¦¬
        if len(self.buffer) > 10000:
            self.buffer = self.buffer[-5000:]

# ì‚¬ìš© ì˜ˆì œ
def train_rl_agent():
    """RL ì—ì´ì „íŠ¸ í›ˆë ¨"""
    
    # í™˜ê²½ ì„¤ì •
    robot = XLeRobot()
    
    # ìƒíƒœ ì°¨ì›: joint positions (6) + joint velocities (6) + target position (3)
    state_dim = 15
    # í–‰ë™ ì°¨ì›: joint torques (6)
    action_dim = 6
    
    agent = PPOAgent(state_dim, action_dim)
    
    num_episodes = 1000
    
    for episode in range(num_episodes):
        state = robot.reset()
        episode_reward = 0
        
        for step in range(200):
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state)
            
            # í™˜ê²½ì— ì ìš©
            next_state, reward, done = robot.step(action)
            
            # ê²½í—˜ ì €ì¥
            agent.store_transition(state, action, reward, next_state, done)
            
            # í•™ìŠµ
            if step % 10 == 0:
                agent.update()
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        if episode % 10 == 0:
            print(f"Episode {episode}: Reward = {episode_reward:.2f}")
```

**ì£¼ìš” RL ì•Œê³ ë¦¬ì¦˜**:
- PPO (Proximal Policy Optimization)
- SAC (Soft Actor-Critic)
- TD3 (Twin Delayed DDPG)
- DDPG (Deep Deterministic Policy Gradient)

**ì¶”ì²œ ìë£Œ**:
- OpenAI Spinning Up: https://spinningup.openai.com/
- Stable Baselines3: https://stable-baselines3.readthedocs.io/
- CleanRL: https://github.com/vwxyzjn/cleanrl

---

### 1.2 ëª¨ë°© í•™ìŠµ (Imitation Learning)

**Behavioral Cloning**

ì‹œì—° ë°ì´í„°ë¡œë¶€í„° ì •ì±… í•™ìŠµ:

```python
class BehavioralCloningAgent:
    """í–‰ë™ ë³µì œ ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_dim, action_dim):
        self.policy = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()
        )
        
        self.optimizer = torch.optim.Adam(
            self.policy.parameters(), 
            lr=1e-3,
            weight_decay=1e-5
        )
    
    def train(self, demonstrations, epochs=100):
        """ì‹œì—° ë°ì´í„°ë¡œ í›ˆë ¨"""
        
        # demonstrations: [(state, action), ...]
        states = torch.FloatTensor([d[0] for d in demonstrations])
        actions = torch.FloatTensor([d[1] for d in demonstrations])
        
        dataset = torch.utils.data.TensorDataset(states, actions)
        dataloader = torch.utils.data.DataLoader(
            dataset, 
            batch_size=64, 
            shuffle=True
        )
        
        for epoch in range(epochs):
            total_loss = 0
            
            for batch_states, batch_actions in dataloader:
                # ì˜ˆì¸¡
                predicted_actions = self.policy(batch_states)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = nn.MSELoss()(predicted_actions, batch_actions)
                
                # ì—­ì „íŒŒ
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 10 == 0:
                avg_loss = total_loss / len(dataloader)
                print(f"Epoch {epoch}: Loss = {avg_loss:.4f}")
    
    def predict(self, state):
        """í–‰ë™ ì˜ˆì¸¡"""
        state = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action = self.policy(state)
        
        return action.squeeze(0).numpy()

# ì‹œì—° ë°ì´í„° ìˆ˜ì§‘
def collect_demonstrations():
    """ì „ë¬¸ê°€ ì‹œì—° ìˆ˜ì§‘"""
    
    robot = XLeRobot()
    demonstrations = []
    
    print("ğŸ® ì‹œì—° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("VR ì»¨íŠ¸ë¡¤ëŸ¬ ë˜ëŠ” ì¡°ì´ìŠ¤í‹±ìœ¼ë¡œ ë¡œë´‡ì„ ì œì–´í•˜ì„¸ìš”")
    
    episode = 0
    
    while True:
        state = robot.get_state()
        
        # ì‚¬ìš©ì ì…ë ¥ (VR, ì¡°ì´ìŠ¤í‹± ë“±)
        action = get_user_input()
        
        # ê¸°ë¡
        demonstrations.append((state, action))
        
        # ë¡œë´‡ ì œì–´
        robot.apply_action(action)
        
        # ì¢…ë£Œ ì¡°ê±´
        if len(demonstrations) >= 1000:
            break
    
    print(f"âœ… {len(demonstrations)}ê°œ ì‹œì—° ìˆ˜ì§‘ ì™„ë£Œ")
    
    return demonstrations
```

**ê³ ê¸‰ ëª¨ë°© í•™ìŠµ**:
- DAgger (Dataset Aggregation)
- GAIL (Generative Adversarial Imitation Learning)
- IRL (Inverse Reinforcement Learning)

---

### 1.3 Vision-Language-Action Models

**VLA (Vision-Language-Action)**

ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í†µí•©:

```python
from transformers import AutoModel, AutoTokenizer
import torch

class VLAController:
    """Vision-Language-Action ì»¨íŠ¸ë¡¤ëŸ¬"""
    
    def __init__(self):
        # ì‚¬ì „í•™ìŠµëœ VLA ëª¨ë¸ ë¡œë“œ
        # ì˜ˆ: RT-2, PaLM-E, OpenVLA ë“±
        
        self.model_name = "openvla/openvla-7b"
        self.model = AutoModel.from_pretrained(self.model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
    
    def predict_action(self, image, instruction):
        """ì´ë¯¸ì§€ì™€ ìì—°ì–´ ëª…ë ¹ìœ¼ë¡œ í–‰ë™ ì˜ˆì¸¡"""
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_inputs = self.tokenizer(
            instruction,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(self.device)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        from torchvision import transforms
        
        preprocess = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
        
        image_tensor = preprocess(image).unsqueeze(0).to(self.device)
        
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(
                pixel_values=image_tensor,
                input_ids=text_inputs['input_ids'],
                attention_mask=text_inputs['attention_mask']
            )
        
        # í–‰ë™ ë””ì½”ë”©
        action = outputs.logits.cpu().numpy()[0]
        
        return action

# ì‚¬ìš© ì˜ˆì œ
def natural_language_control():
    """ìì—°ì–´ ê¸°ë°˜ ë¡œë´‡ ì œì–´"""
    
    robot = XLeRobot()
    camera = RGBDCamera()
    controller = VLAController()
    
    instructions = [
        "Pick up the red apple",
        "Place it in the blue bowl",
        "Clean the table",
        "Navigate to the kitchen",
    ]
    
    for instruction in instructions:
        print(f"\nğŸ“ ëª…ë ¹: {instruction}")
        
        # í˜„ì¬ ì´ë¯¸ì§€ íšë“
        image = camera.get_rgb()
        
        # í–‰ë™ ì˜ˆì¸¡
        action = controller.predict_action(image, instruction)
        
        # í–‰ë™ ì‹¤í–‰
        robot.execute_action(action)
        
        print("âœ… ì™„ë£Œ")
```

**ê´€ë ¨ ëª¨ë¸**:
- RT-2 (Robotics Transformer 2)
- PaLM-E (Embodied Language Model)
- OpenVLA (Open Vision-Language-Action)
- CLIP + Policy Network

---

### 1.4 Multi-Agent Coordination

**ë‹¤ì¤‘ ë¡œë´‡ í˜‘ì—…**

```python
class MultiAgentSystem:
    """ë‹¤ì¤‘ ë¡œë´‡ í˜‘ì—… ì‹œìŠ¤í…œ"""
    
    def __init__(self, num_robots):
        self.robots = [XLeRobot(id=i) for i in range(num_robots)]
        self.num_robots = num_robots
        
        # ì¤‘ì•™ ì¡°ì •ì
        self.coordinator = CentralCoordinator()
        
        # í†µì‹ 
        self.message_queue = []
    
    def assign_tasks(self, tasks):
        """ì‘ì—… í• ë‹¹"""
        
        # ì‘ì—…-ë¡œë´‡ ë§¤ì¹­ ìµœì í™”
        assignments = self.coordinator.optimize_assignment(
            tasks, self.robots
        )
        
        for robot_id, task in assignments.items():
            self.robots[robot_id].assign_task(task)
    
    def execute_collaborative_task(self, task):
        """í˜‘ì—… ì‘ì—… ì‹¤í–‰"""
        
        if task.type == "carry_heavy_object":
            # 2ê°œ ë¡œë´‡ìœ¼ë¡œ ë¬´ê±°ìš´ ë¬¼ì²´ ìš´ë°˜
            robot1, robot2 = self.robots[0], self.robots[1]
            
            # ë™ê¸°í™”ëœ ê·¸ë¦½
            robot1.move_to(task.object_position)
            robot2.move_to(task.object_position + np.array([0.3, 0, 0]))
            
            robot1.close_gripper()
            robot2.close_gripper()
            
            # ë™ê¸°í™”ëœ ì´ë™
            self.synchronized_move([robot1, robot2], task.target_position)
            
            robot1.open_gripper()
            robot2.open_gripper()
    
    def synchronized_move(self, robots, target):
        """ë™ê¸°í™”ëœ ì´ë™"""
        
        # ê° ë¡œë´‡ì˜ ê²½ë¡œ ê³„íš
        paths = []
        for robot in robots:
            path = robot.plan_path(target)
            paths.append(path)
        
        # ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° ë™ê¸°í™”
        max_len = max(len(p) for p in paths)
        
        for step in range(max_len):
            for i, robot in enumerate(robots):
                if step < len(paths[i]):
                    robot.move_to(paths[i][step])
            
            # ëŒ€ê¸° (ë™ê¸°í™”)
            time.sleep(0.1)

class CentralCoordinator:
    """ì¤‘ì•™ ì¡°ì • ì‹œìŠ¤í…œ"""
    
    def optimize_assignment(self, tasks, robots):
        """ìµœì  ì‘ì—… í• ë‹¹"""
        
        # í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ ë˜ëŠ” greedy í• ë‹¹
        
        assignments = {}
        
        for i, task in enumerate(tasks):
            # ê°€ì¥ ê°€ê¹Œìš´ ë¡œë´‡ ì°¾ê¸°
            best_robot = min(
                range(len(robots)),
                key=lambda r: np.linalg.norm(
                    robots[r].position - task.location
                )
            )
            
            assignments[best_robot] = task
        
        return assignments
```

---

## 2. í”„ë¡œì íŠ¸ ê°œì„  ë°©í–¥

### 2.1 ì„±ëŠ¥ ìµœì í™”

**ì‹¤ì‹œê°„ ì„±ëŠ¥ í–¥ìƒ**

```python
# 1. C++ í™•ì¥ ëª¨ë“ˆ ì‚¬ìš©
# pybind11ë¡œ ì„±ëŠ¥ criticalí•œ ë¶€ë¶„ì„ C++ë¡œ êµ¬í˜„

# 2. GPU ê°€ì†
import cupy as cp  # NumPyì˜ GPU ë²„ì „

def accelerated_computation():
    # NumPy ë°°ì—´ì„ CuPy ë°°ì—´ë¡œ
    data_gpu = cp.array(data_cpu)
    
    # GPUì—ì„œ ê³„ì‚°
    result_gpu = cp.dot(data_gpu, matrix_gpu)
    
    # CPUë¡œ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
    result_cpu = cp.asnumpy(result_gpu)

# 3. ë©€í‹°í”„ë¡œì„¸ì‹±
from multiprocessing import Pool

def parallel_processing():
    with Pool(processes=4) as pool:
        results = pool.map(process_image, image_list)

# 4. JIT ì»´íŒŒì¼ (Numba)
from numba import jit

@jit(nopython=True)
def fast_computation(array):
    result = 0.0
    for i in range(len(array)):
        result += array[i] ** 2
    return result
```

### 2.2 ì•ˆì •ì„± í–¥ìƒ

**Fault Tolerance**

```python
class RobustRobotController:
    """ê²¬ê³ í•œ ë¡œë´‡ ì»¨íŠ¸ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.robot = XLeRobot()
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§
        self.health_monitor = HealthMonitor()
        
        # ë°±ì—… ì‹œìŠ¤í…œ
        self.backup_sensors = BackupSensors()
        
    def execute_with_recovery(self, task):
        """ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ê³¼ í•¨ê»˜ ì‹¤í–‰"""
        
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                # ì‚¬ì „ ì²´í¬
                if not self.health_monitor.is_healthy():
                    self.perform_diagnostics()
                    continue
                
                # ì‘ì—… ì‹¤í–‰
                result = task.execute()
                
                # ì‚¬í›„ ì²´í¬
                if self.verify_result(result):
                    return result
                else:
                    print(f"âš ï¸ ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨, ì¬ì‹œë„ {attempt + 1}")
                    
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
                # ì˜¤ë¥˜ ë³µêµ¬
                self.recover_from_error(e)
        
        # ìµœì¢… ì‹¤íŒ¨
        self.enter_safe_mode()
        return None
    
    def recover_from_error(self, error):
        """ì˜¤ë¥˜ ë³µêµ¬"""
        
        if isinstance(error, SensorFailure):
            # ë°±ì—… ì„¼ì„œë¡œ ì „í™˜
            self.switch_to_backup_sensors()
        
        elif isinstance(error, MotorStall):
            # ëª¨í„° ë¦¬ì…‹
            self.reset_motors()
        
        elif isinstance(error, CollisionDetected):
            # ì¶©ëŒ í›„ ë³µêµ¬
            self.retreat_and_replan()
    
    def enter_safe_mode(self):
        """ì•ˆì „ ëª¨ë“œ ì§„ì…"""
        
        # ëª¨ë“  ë™ì‘ ì¤‘ì§€
        self.robot.stop()
        
        # ì•ˆì „ ìì„¸ë¡œ ì´ë™
        self.robot.move_to_home_position()
        
        # ê²½ê³  ë°œìƒ
        self.send_alert("ë¡œë´‡ì´ ì•ˆì „ ëª¨ë“œì— ì§„ì…í–ˆìŠµë‹ˆë‹¤")
```

### 2.3 í™•ì¥ì„±

**Modular Architecture**

```python
# í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ
class PluginManager:
    """í”ŒëŸ¬ê·¸ì¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.plugins = {}
    
    def register_plugin(self, name, plugin):
        """í”ŒëŸ¬ê·¸ì¸ ë“±ë¡"""
        self.plugins[name] = plugin
        print(f"âœ… í”ŒëŸ¬ê·¸ì¸ ë“±ë¡: {name}")
    
    def get_plugin(self, name):
        """í”ŒëŸ¬ê·¸ì¸ ê°€ì ¸ì˜¤ê¸°"""
        return self.plugins.get(name)

# ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
class CustomGripperPlugin:
    """ì»¤ìŠ¤í…€ ê·¸ë¦¬í¼ í”ŒëŸ¬ê·¸ì¸"""
    
    def __init__(self):
        self.name = "custom_gripper"
    
    def grasp(self, object_info):
        # ì»¤ìŠ¤í…€ ê·¸ë¦½ ë¡œì§
        pass

# ì‚¬ìš©
plugin_manager = PluginManager()
plugin_manager.register_plugin("gripper", CustomGripperPlugin())
```

---

## 3. ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬

### 3.1 ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ ë°©ë²•

**GitHub Workflow**

1. **ì´ìŠˆ ì°¾ê¸°**
   - Good First Issue
   - Help Wanted
   - Bug Reports

2. **Fork & Clone**
```bash
# Fork
gh repo fork trossen-robotics/XLeRobot

# Clone
git clone https://github.com/YOUR_USERNAME/XLeRobot.git
cd XLeRobot

# Upstream ì¶”ê°€
git remote add upstream https://github.com/trossen-robotics/XLeRobot.git
```

3. **ë¸Œëœì¹˜ ìƒì„±**
```bash
git checkout -b feature/your-feature-name
```

4. **ì½”ë“œ ì‘ì„± ë° í…ŒìŠ¤íŠ¸**
```bash
# ì½”ë“œ ìˆ˜ì •
vim software/src/robots/xlerobot.py

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/

# ë¦°íŠ¸ ì²´í¬
flake8 software/
black software/
```

5. **ì»¤ë°‹ ë° í‘¸ì‹œ**
```bash
git add .
git commit -m "feat: Add new gripper control method"
git push origin feature/your-feature-name
```

6. **Pull Request ìƒì„±**
   - ëª…í™•í•œ ì œëª©ê³¼ ì„¤ëª…
   - ë³€ê²½ ì‚¬í•­ ìš”ì•½
   - í…ŒìŠ¤íŠ¸ ê²°ê³¼ í¬í•¨

### 3.2 ë¬¸ì„œí™”

**ì¢‹ì€ ë¬¸ì„œ ì‘ì„±ë²•**

```markdown
# Feature Name

## ê°œìš”
ê¸°ëŠ¥ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…

## ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‚¬ìš©
\`\`\`python
from xlerobot import NewFeature

feature = NewFeature()
result = feature.execute()
\`\`\`

### ê³ ê¸‰ ì‚¬ìš©
\`\`\`python
# íŒŒë¼ë¯¸í„° ì„¤ëª…
feature = NewFeature(
    param1=value1,  # param1 ì„¤ëª…
    param2=value2   # param2 ì„¤ëª…
)
\`\`\`

## API ë ˆí¼ëŸ°ìŠ¤

### `NewFeature(param1, param2)`

**Parameters:**
- `param1` (type): ì„¤ëª…
- `param2` (type): ì„¤ëª…

**Returns:**
- `result` (type): ì„¤ëª…

**Example:**
\`\`\`python
...
\`\`\`

## ì£¼ì˜ì‚¬í•­
- ì£¼ì˜ì  1
- ì£¼ì˜ì  2
```

### 3.3 íŠœí† ë¦¬ì–¼ ì‘ì„±

**íš¨ê³¼ì ì¸ íŠœí† ë¦¬ì–¼ êµ¬ì¡°**

1. **ëª©í‘œ ëª…ì‹œ**: í•™ìŠµìê°€ ë¬´ì—‡ì„ ë°°ìš¸ì§€
2. **ì „ì œ ì¡°ê±´**: í•„ìš”í•œ ì„ ìˆ˜ ì§€ì‹
3. **ë‹¨ê³„ë³„ ì„¤ëª…**: ëª…í™•í•˜ê³  ë”°ë¼í•˜ê¸° ì‰¬ìš´ ë‹¨ê³„
4. **ì½”ë“œ ì˜ˆì œ**: ì‹¤í–‰ ê°€ëŠ¥í•œ ì™„ì „í•œ ì˜ˆì œ
5. **ë¬¸ì œ í•´ê²°**: ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²° ë°©ë²•
6. **ë‹¤ìŒ ë‹¨ê³„**: ì¶”ê°€ í•™ìŠµ ìë£Œ

---

## 4. ì¶”ê°€ í•™ìŠµ ìë£Œ

### 4.1 ì¶”ì²œ ê°•ì˜

**ì˜¨ë¼ì¸ ê°•ì˜**
- ğŸ“ Modern Robotics (Coursera)
- ğŸ“ CS287: Advanced Robotics (UC Berkeley)
- ğŸ“ Robot Learning (Stanford)
- ğŸ“ Deep RL Bootcamp (Berkeley)

**YouTube ì±„ë„**
- ğŸ“º Two Minute Papers
- ğŸ“º Lex Fridman
- ğŸ“º Robotics Today

### 4.2 ì¶”ì²œ ë„ì„œ

ğŸ“š **ë¡œë´‡ ê³µí•™ ê¸°ì´ˆ**
- "Robotics: Modelling, Planning and Control" - Siciliano et al.
- "Modern Robotics" - Lynch & Park
- "Introduction to Robotics" - Craig

ğŸ“š **ë¨¸ì‹ ëŸ¬ë‹ & AI**
- "Deep Learning" - Goodfellow et al.
- "Reinforcement Learning" - Sutton & Barto
- "Pattern Recognition and Machine Learning" - Bishop

ğŸ“š **ì»´í“¨í„° ë¹„ì „**
- "Computer Vision: Algorithms and Applications" - Szeliski
- "Multiple View Geometry" - Hartley & Zisserman

### 4.3 ì—°êµ¬ ë…¼ë¬¸

**í•„ìˆ˜ ë…¼ë¬¸**

1. **Manipulation**
   - "Learning Synergies between Pushing and Grasping"
   - "Dex-Net: Deep Learning to Plan Grasps"

2. **Navigation**
   - "ORB-SLAM: A Versatile and Accurate Monocular SLAM"
   - "TEB: Timed Elastic Band Local Planning"

3. **Learning**
   - "Deep Reinforcement Learning for Robotic Manipulation"
   - "Learning Dexterous In-Hand Manipulation"

4. **Vision-Language**
   - "RT-2: Vision-Language-Action Models"
   - "PaLM-E: An Embodied Multimodal Language Model"

**ë…¼ë¬¸ ê²€ìƒ‰**
- Google Scholar
- arXiv.org (cs.RO, cs.AI, cs.CV)
- Papers with Code

---

## 5. ì‹¤ì „ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´

### 5.1 ì´ˆê¸‰ í”„ë¡œì íŠ¸

1. **ğŸ¯ ë¬¼ì²´ ë¶„ë¥˜ ë¡œë´‡**
   - YOLOë¡œ ë¬¼ì²´ ê°ì§€
   - ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
   - ì ì ˆí•œ ìœ„ì¹˜ì— ë°°ì¹˜

2. **ğŸ§¹ ì²­ì†Œ ë¡œë´‡**
   - í…Œì´ë¸”/ë°”ë‹¥ ìŠ¤ìº”
   - ì“°ë ˆê¸° ê°ì§€
   - ì“°ë ˆê¸°í†µìœ¼ë¡œ ì´ë™

3. **ğŸ“¦ ì°½ê³  ë„ìš°ë¯¸**
   - QR ì½”ë“œë¡œ ë¬¼í’ˆ ì¸ì‹
   - ì„ ë°˜ì—ì„œ í”½ì—…
   - ì§€ì • ìœ„ì¹˜ë¡œ ì´ë™

### 5.2 ì¤‘ê¸‰ í”„ë¡œì íŠ¸

1. **ğŸ½ï¸ ì‹ë‹¹ ì„œë¹™ ë¡œë´‡**
   - ì£¼ë¬¸ ìŒì„± ì¸ì‹
   - ì£¼ë°©ì—ì„œ í”½ì—…
   - í…Œì´ë¸”ë¡œ ë„¤ë¹„ê²Œì´ì…˜
   - ì•ˆì „í•œ ì „ë‹¬

2. **ğŸ¥ ë³‘ì› ë³´ì¡° ë¡œë´‡**
   - ì•½í’ˆ/ì¥ë¹„ ìš´ë°˜
   - í™˜ì ìœ„ì¹˜ ì¶”ì 
   - ê°„í˜¸ì‚¬ í˜¸ì¶œ ëŒ€ì‘

3. **ğŸ­ ì œì¡° ë¼ì¸ ë³´ì¡°**
   - ë¶€í’ˆ ê²€ì‚¬ (ë¹„ì „)
   - ë¶ˆëŸ‰í’ˆ ë¶„ë¥˜
   - ì¬ê³  ê´€ë¦¬

### 5.3 ê³ ê¸‰ í”„ë¡œì íŠ¸

1. **ğŸ  ì™„ì „ ììœ¨ ê°€ì • ë¡œë´‡**
   - ìì—°ì–´ ëª…ë ¹ ì´í•´
   - ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰
   - í•™ìŠµ ë° ì ì‘
   - ì•ˆì „ ë³´ì¥

2. **ğŸ¤ ì¸ê°„-ë¡œë´‡ í˜‘ì—… ì‹œìŠ¤í…œ**
   - ì˜ë„ ì˜ˆì¸¡
   - ë™ê¸°í™”ëœ ì‘ì—…
   - ì‹¤ì‹œê°„ ì ì‘

3. **ğŸ§ª ì—°êµ¬ìš© í”Œë«í¼**
   - ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
   - ë²¤ì¹˜ë§ˆí¬ êµ¬ì¶•
   - ë…¼ë¬¸ ì¬í˜„

---

## 6. ê²½ì§„ëŒ€íšŒ & ì´ë²¤íŠ¸

### 6.1 ì£¼ìš” ë¡œë´‡ ê²½ì§„ëŒ€íšŒ

**êµ­ì œ ëŒ€íšŒ**
- ğŸ† RoboCup
- ğŸ† Amazon Robotics Challenge
- ï¿½ï¿½ DARPA Robotics Challenge
- ğŸ† World Robot Summit

**êµ­ë‚´ ëŒ€íšŒ**
- ğŸ‡°ğŸ‡· ì§€ëŠ¥í˜• ë¡œë´‡ ê²½ì§„ëŒ€íšŒ
- ğŸ‡°ğŸ‡· ë¡œë´‡ ì†Œí”„íŠ¸ì›¨ì–´ ê²½ì§„ëŒ€íšŒ

### 6.2 ì»¨í¼ëŸ°ìŠ¤

**ì£¼ìš” ì»¨í¼ëŸ°ìŠ¤**
- ICRA (International Conference on Robotics and Automation)
- IROS (Intelligent Robots and Systems)
- RSS (Robotics: Science and Systems)
- CoRL (Conference on Robot Learning)

**ì°¸ê°€ ë°©ë²•**
1. ë…¼ë¬¸ íˆ¬ê³ 
2. ì›Œí¬ìƒµ ì°¸ê°€
3. ë°ëª¨ ì„¸ì…˜
4. ë„¤íŠ¸ì›Œí‚¹

---

## 7. ì§„ë¡œ ë° ì»¤ë¦¬ì–´

### 7.1 ë¡œë´‡ ê´€ë ¨ ì§ì—…

**ì—°êµ¬ ë¶„ì•¼**
- ğŸ”¬ ì—°êµ¬ì› (ëŒ€í•™, ì—°êµ¬ì†Œ)
- ğŸ”¬ AI/ML ì—”ì§€ë‹ˆì–´
- ğŸ”¬ ë¡œë´‡ ë¹„ì „ ì „ë¬¸ê°€

**ì‚°ì—… ë¶„ì•¼**
- ğŸ­ ë¡œë´‡ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´
- ğŸ­ ììœ¨ì£¼í–‰ ì—”ì§€ë‹ˆì–´
- ğŸ­ ì œì–´ ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´

**ì°½ì—…**
- ğŸ’¡ ë¡œë´‡ ìŠ¤íƒ€íŠ¸ì—…
- ğŸ’¡ ìë™í™” ì†”ë£¨ì…˜
- ğŸ’¡ ì„œë¹„ìŠ¤ ë¡œë´‡

### 7.2 í•„ìš”í•œ ìŠ¤í‚¬

**ê¸°ìˆ  ìŠ¤í‚¬**
- âœ… í”„ë¡œê·¸ë˜ë° (Python, C++)
- âœ… ìˆ˜í•™ (ì„ í˜•ëŒ€ìˆ˜, ë¯¸ì ë¶„, í™•ë¥ )
- âœ… ì œì–´ ì´ë¡ 
- âœ… ì»´í“¨í„° ë¹„ì „
- âœ… ë¨¸ì‹ ëŸ¬ë‹

**ì†Œí”„íŠ¸ ìŠ¤í‚¬**
- âœ… ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
- âœ… í˜‘ì—… ëŠ¥ë ¥
- âœ… ì˜ì‚¬ì†Œí†µ ëŠ¥ë ¥
- âœ… ì§€ì†ì  í•™ìŠµ

---

## 8. ì»¤ë®¤ë‹ˆí‹° ë¦¬ì†ŒìŠ¤

### 8.1 ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°

**í¬ëŸ¼ & í† ë¡ **
- ğŸ’¬ ROS Discourse
- ğŸ’¬ Reddit r/robotics
- ğŸ’¬ Stack Overflow

**ì±„íŒ…**
- ğŸ’¬ ROS Discord
- ğŸ’¬ Robotics Slack

**í•œêµ­ ì»¤ë®¤ë‹ˆí‹°**
- ğŸ‡°ï¿½ï¿½ ë¡œë´‡ì‹ ë¬¸
- ğŸ‡°ğŸ‡· ë¡œë³´í‹±ìŠ¤ ì—°êµ¬íšŒ

### 8.2 ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸

**ì¶”ì²œ í”„ë¡œì íŠ¸**
- ğŸ¤– ROS / ROS2
- ğŸ¤– MoveIt
- ğŸ¤– OpenCV
- ğŸ¤– PyTorch / TensorFlow
- ğŸ¤– IsaacGym / MuJoCo

### 8.3 ë°ì´í„°ì…‹

**ê³µê°œ ë°ì´í„°ì…‹**
- ğŸ“Š RoboNet
- ğŸ“Š Google Robot Dataset
- ğŸ“Š Berkeley Autolab
- ğŸ“Š COCO (for vision)

---

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

XLeRobot í•™ìŠµ ê°€ì´ë“œë¥¼ ëª¨ë‘ ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤!

### âœ¨ ë‹¹ì‹ ì´ ë°°ìš´ ê²ƒ

1. **Chapter 1-2**: í•˜ë“œì›¨ì–´ & ì†Œí”„íŠ¸ì›¨ì–´ ê¸°ì´ˆ
2. **Chapter 3**: ìš´ë™í•™ & ì œì–´
3. **Chapter 4**: ì»´í“¨í„° ë¹„ì „
4. **Chapter 5**: ì„¼ì„œ í†µí•©
5. **Chapter 6**: ë™ì‘ ê³„íš
6. **Chapter 7**: í…”ë ˆì˜¤í¼ë ˆì´ì…˜
7. **Chapter 8**: 6ê°œ ì‹¤ì „ í”„ë¡œì íŠ¸

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **ì‹¤ìŠµ**: ë³¸ì¸ë§Œì˜ í”„ë¡œì íŠ¸ ì‹œì‘
2. **ê¸°ì—¬**: ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬
3. **ê³µìœ **: ë°°ìš´ ë‚´ìš©ì„ ë‹¤ë¥¸ ì‚¬ëŒê³¼ ê³µìœ 
4. **í•™ìŠµ**: ì§€ì†ì ìœ¼ë¡œ ìƒˆë¡œìš´ ê¸°ìˆ  í•™ìŠµ

### ğŸ’¡ ë§ˆì§€ë§‰ ì¡°ì–¸

> "The best way to learn robotics is to build robots."
> 
> "ë¡œë´‡ ê³µí•™ì„ ë°°ìš°ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ ë¡œë´‡ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤."

**Keep Building! Keep Learning! Keep Sharing!**

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- XLeRobot GitHub: https://github.com/trossen-robotics/XLeRobot
- ROS Documentation: https://docs.ros.org/
- OpenCV Docs: https://docs.opencv.org/

### í•™ìŠµ í”Œë«í¼
- Coursera: https://www.coursera.org/
- edX: https://www.edx.org/
- Udacity: https://www.udacity.com/

### ë‰´ìŠ¤ & ë¸”ë¡œê·¸
- IEEE Spectrum Robotics: https://spectrum.ieee.org/robotics
- The Robot Report: https://www.therobotreport.com/
- Robotics Business Review: https://www.roboticsbusinessreview.com/

---

## ğŸ™ ê°ì‚¬ì˜ ë§

XLeRobot ì»¤ë®¤ë‹ˆí‹°ì™€ ëª¨ë“  ê¸°ì—¬ìë“¤ì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.

**Happy Roboting! ğŸ¤–**

---

[â† 8.5 ê°€ì •ìš© ì‘ì—…](https://github.com/dinnerandcoffee/xlerobot-learning-guide/blob/main/learning_guide/08_practice_projects/05_household_tasks.md) | [ì²˜ìŒìœ¼ë¡œ â†‘](https://github.com/dinnerandcoffee/xlerobot-learning-guide/blob/main/learning_guide/README.md)
